stage: debug

task:
  name: glue/rte
  max_len: 128

model:
  name: roberta-base
  type: sequence_classification

method:
  name: baseline
  lora:
    r: 8
    R: 32
    alpha: 16
    dropout: 0.1

  ours:
    # legacy field (kept for backward compat; only used when history.enabled=false)
    ema_H: 4

    votes: 8  # legacy; actual voting controlled by voting.samples_per_vote

    # -------------------------
    # FIRST LAYER TRIGGER (主方向门槛)
    # mode:
    #   - percentile: 触发 top% 的 “worst blocks” (by metric badness)
    #   - cos_threshold: 硬阈值触发
    # metric:
    #   - C_r | C_R | A_b | misalign
    #   * C_r/C_R/A_b: 越小越糟（<= tau 触发）
    #   * misalign=1-A_b: 越大越糟（>= tau 触发）
    # -------------------------
    trigger:
      mode: percentile            # percentile | cos_threshold
      metric: C_R                 # C_r | C_R | A_b | misalign
      percentile_p: 90            # only when mode=percentile
      cos_threshold: 0.30         # only when mode=cos_threshold

    # backward compat
    percentile_p: 90

    # gates / correction hypers
    k: 8.0
    gamma_r: 0.3
    gamma_hi: 0.3
    eta: 0.2
    beta_max: 0.5
    rho_max: 0.5
    alpha_min: 0.1
    lambda_n: 0.4
    lambda_o: 0.4

    # -------------------------
    # CORRECTION MODE（只保留两种）
    # direction_only: 只做 beta_r/beta_hi + joint CAGrad（不做 alpha；不 rescale）
    # both: alpha + (beta + CAGrad)（不 rescale）
    # -------------------------
    correction:
      mode: both                  # both | direction_only
      direction_rescale: false    # kept for compat; ignored by controller

    # ---- voting control ----
    voting:
      samples_per_vote: 8        # {16, 8, 4} -> default 8 samples per vote window (batch=32 => 4 windows)
      allow_tail: true
      combine_votes: false
      combine_size: 2
      combine_max: 0
      keep_single_votes: true

    # ---- history / truncated EMA over steps ----
    history:
      enabled: true
      window_steps: 4            # {8,4,2}
      weighting: exp             # exp | uniform | linear
      exp_beta: 0.7

train:
  epochs: 3
  batch_size: 32
  lr: 2e-5
  warmup_ratio: 0.1
  weight_decay: 0.01
  grad_accum: 1
  max_grad_norm: 1.0
  seed: 42
  eval:
    strategy: dense_early
    dense_early_epochs: 2
    dense_early_per_epoch: 8

compute:
  accelerator: cuda
  num_gpus: 1
  gpus_per_trial: 1
  mixed_precision: "fp32"
  compile: false

hpo:
  mode: hierarchical

  lr_grid:
    min_exp: -6
    max_exp: -3
    points: 15

  warmup_grid: [0.06]
  baseline_lr_points: 5

  ours:
    lr_neighbor_points_small: 7
    lr_neighbor_points_large: 5
    stages:
      s1:
        epochs: 1
        seeds: 1
        trials: 100
        param_pool: ["lambda_n", "lambda_o", "eta", "k", "gamma_hi"]
        candidates_per_param: 4
        keep_top: 2
      s2:
        epochs: 2
        seeds: 1
        trials: 40
        combine_topk_params: 3
      s3:
        epochs: 4
        seeds_small: 2
        seeds_large: 1
        trials_small: 120
        trials_large: 90
        score:
          w_max: 0.5
          w_final: 0.4
          w_avg: 0.1

io:
  root: runs
  overwrite: ask
  save_best_hparams_json: true

log:
  csv: true
  swanlab:
    enabled: false
    project: "ShakeAlign_LoRA"
    run_group: null

debug:
  enabled: true
  print_every_steps: 20
  max_blocks_to_print: 3
  dump_init: true
  dump_votes: true
  dump_gates: true
  dump_grad_norms: true
  dump_history: false
  assert_hi_zero_init: true
